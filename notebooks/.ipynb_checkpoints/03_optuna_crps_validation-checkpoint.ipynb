{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with CRPS and Train/Test Validation\n",
    "\n",
    "This notebook demonstrates optimizing MMM hyperparameters using Optuna with **CRPS (Continuous Ranked Probability Score)** on a train/test split:\n",
    "- **Training set**: First 80 weeks (weeks 1-80)\n",
    "- **Test set**: Last 24 weeks (weeks 81-104)\n",
    "- **Hyperparameters**: `yearly_seasonality` (1-10) and `adstock_max_lag` (4-12 weeks)\n",
    "\n",
    "## Why CRPS?\n",
    "\n",
    "Unlike WAIC (which evaluates in-sample fit), CRPS evaluates out-of-sample predictive performance:\n",
    "- **WAIC**: Measures model fit on training data (penalized for complexity)\n",
    "- **CRPS**: Measures prediction accuracy on held-out test data\n",
    "- **Advantage**: Direct assessment of generalization performance\n",
    "\n",
    "CRPS is a proper scoring rule that evaluates the entire predictive distribution, not just point estimates. Lower CRPS = better predictions.\n",
    "\n",
    "## Optimization Strategy\n",
    "\n",
    "1. **Train/test split**: 80 weeks train, 24 weeks test (no data leakage)\n",
    "2. **Sequential trials** (no parallel optimization)\n",
    "3. **NumPyro sampler** for 2-10x speedup\n",
    "4. **Convergence checks** on training set\n",
    "5. **CRPS evaluation** on test set using posterior predictive\n",
    "6. **Reduced MCMC** during optimization (500 draws/tune)\n",
    "7. **Final refit** on full data with optimal parameters (2000 draws/tune)\n",
    "8. **Optuna pruning** for failed convergence\n",
    "\n",
    "## Implementation Notes\n",
    "\n",
    "- `pymc_marketing.metrics.crps()` expects `y_pred` with shape (n_samples, n_observations)\n",
    "- Posterior predictive samples provide the probabilistic predictions\n",
    "- NumPyro progress bar disabled in Jupyter to avoid kernel issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from pymc_marketing.mmm import MMM, GeometricAdstock, LogisticSaturation\n",
    "from pymc_marketing.metrics import crps\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from rich.table import Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "TEST_SIZE_WEEKS = 24  # Last 24 weeks for testing\n",
    "\n",
    "# Optimization settings\n",
    "N_TRIALS = 20\n",
    "OPTUNA_DRAWS = 500\n",
    "OPTUNA_TUNE = 500\n",
    "OPTUNA_CHAINS = 2\n",
    "\n",
    "# Final model settings\n",
    "FINAL_DRAWS = 2000\n",
    "FINAL_TUNE = 2000\n",
    "FINAL_CHAINS = 4\n",
    "\n",
    "# Sampler selection\n",
    "USE_NUMPYRO = True\n",
    "NUTS_SAMPLER = \"numpyro\" if USE_NUMPYRO else \"pymc\"\n",
    "\n",
    "# Convergence thresholds for optimization (lenient)\n",
    "OPTUNA_DIVERGENCE_THRESHOLD = 0.10\n",
    "OPTUNA_RHAT_THRESHOLD = 1.10\n",
    "OPTUNA_ESS_THRESHOLD = 50\n",
    "\n",
    "# Convergence thresholds for final model (strict)\n",
    "FINAL_DIVERGENCE_THRESHOLD = 0.01\n",
    "FINAL_RHAT_THRESHOLD = 1.01\n",
    "FINAL_ESS_THRESHOLD = 400\n",
    "\n",
    "# Random seed\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "rprint(f\"[bold blue]Configuration:[/bold blue]\")\n",
    "rprint(f\"Train/test split: {TEST_SIZE_WEEKS} weeks for test\")\n",
    "rprint(f\"Number of trials: {N_TRIALS}\")\n",
    "rprint(f\"NUTS sampler: [yellow]{NUTS_SAMPLER}[/yellow] {'(NumPyro - faster!)' if USE_NUMPYRO else '(PyMC default)'}\")\n",
    "rprint(f\"Optimization MCMC: {OPTUNA_DRAWS} draws, {OPTUNA_TUNE} tune, {OPTUNA_CHAINS} chains\")\n",
    "rprint(f\"Final model MCMC: {FINAL_DRAWS} draws, {FINAL_TUNE} tune, {FINAL_CHAINS} chains\")\n",
    "rprint(f\"\\n[bold yellow]Convergence Thresholds:[/bold yellow]\")\n",
    "rprint(f\"Optimization - Divergences: <{OPTUNA_DIVERGENCE_THRESHOLD*100:.0f}%, R-hat: <{OPTUNA_RHAT_THRESHOLD}, ESS: >{OPTUNA_ESS_THRESHOLD}\")\n",
    "rprint(f\"Final model  - Divergences: <{FINAL_DIVERGENCE_THRESHOLD*100:.0f}%, R-hat: <{FINAL_RHAT_THRESHOLD}, ESS: >{FINAL_ESS_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mmm_data(data_path: str | Path) -> pl.DataFrame:\n",
    "    \"\"\"Load MMM data from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the mmm_data.csv file\n",
    "        \n",
    "    Returns:\n",
    "        Polars DataFrame with parsed date column\n",
    "    \"\"\"\n",
    "    return pl.read_csv(data_path).with_columns(\n",
    "        pl.col(\"date\").str.to_date()\n",
    "    )\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data_path = Path(\"../data/mmm_data.csv\")\n",
    "df = load_mmm_data(data_path)\n",
    "\n",
    "rprint(f\"[bold green]Data loaded successfully[/bold green]\")\n",
    "rprint(f\"Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "rprint(f\"Date range: {df['date'].min()} to {df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "Split data chronologically: first 80 weeks for training, last 24 weeks for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(\n",
    "    df: pl.DataFrame,\n",
    "    test_size_weeks: int\n",
    ") -> tuple[pl.DataFrame, pl.DataFrame]:\n",
    "    \"\"\"Split data into train and test sets chronologically.\n",
    "    \n",
    "    Args:\n",
    "        df: Full dataset\n",
    "        test_size_weeks: Number of weeks to use for test set\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_df, test_df)\n",
    "    \"\"\"\n",
    "    n_total = df.shape[0]\n",
    "    n_train = n_total - test_size_weeks\n",
    "    \n",
    "    # Sort by date to ensure chronological order\n",
    "    df_sorted = df.sort(\"date\")\n",
    "    \n",
    "    train_df = df_sorted[:n_train]\n",
    "    test_df = df_sorted[n_train:]\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "# Split data\n",
    "df_train, df_test = split_train_test(df, TEST_SIZE_WEEKS)\n",
    "\n",
    "rprint(\"\\n[bold blue]Train/Test Split:[/bold blue]\")\n",
    "rprint(f\"Train set: {df_train.shape[0]} weeks ({df_train['date'].min()} to {df_train['date'].max()})\")\n",
    "rprint(f\"Test set:  {df_test.shape[0]} weeks ({df_test['date'].min()} to {df_test['date'].max()})\")\n",
    "\n",
    "# Convert to pandas for PyMC-Marketing\n",
    "df_train_pandas = df_train.to_pandas()\n",
    "df_test_pandas = df_test.to_pandas()\n",
    "\n",
    "# Define model inputs\n",
    "channel_columns = [\n",
    "    \"x1_Search-Ads\",\n",
    "    \"x2_Social-Media\",\n",
    "    \"x3_Local-Ads\",\n",
    "    \"x4_Email\"\n",
    "]\n",
    "\n",
    "control_columns = [\"c1\", \"c2\"]\n",
    "\n",
    "# Prepare train/test splits\n",
    "X_train = df_train_pandas.drop(columns=[\"y\"])\n",
    "y_train = df_train_pandas[\"y\"]\n",
    "\n",
    "X_test = df_test_pandas.drop(columns=[\"y\"])\n",
    "y_test = df_test_pandas[\"y\"]\n",
    "\n",
    "rprint(\"\\n[bold blue]Model Configuration:[/bold blue]\")\n",
    "rprint(f\"Target column: [yellow]y[/yellow]\")\n",
    "rprint(f\"Date column: [yellow]date[/yellow]\")\n",
    "rprint(f\"Channel columns: [yellow]{channel_columns}[/yellow]\")\n",
    "rprint(f\"Control columns: [yellow]{control_columns}[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Diagnostics\n",
    "\n",
    "Helper function to check MCMC convergence quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\n",
    "def check_convergence(\n",
    "    mmm: MMM,\n",
    "    divergence_threshold: float,\n",
    "    rhat_threshold: float,\n",
    "    ess_threshold: float,\n",
    "    trial_number: int | None = None\n",
    ") -> tuple[bool, dict[str, float]]:\n",
    "    \"\"\"Check MCMC convergence diagnostics.\n",
    "    \n",
    "    Args:\n",
    "        mmm: Fitted MMM model\n",
    "        divergence_threshold: Maximum allowed divergence rate (0-1)\n",
    "        rhat_threshold: Maximum allowed R-hat value\n",
    "        ess_threshold: Minimum required effective sample size\n",
    "        trial_number: Optional trial number for logging\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (converged: bool, diagnostics: dict)\n",
    "    \"\"\"\n",
    "    # 1. Check divergences\n",
    "    n_divergences = int(mmm.idata.sample_stats.diverging.sum().item())\n",
    "    n_draws = mmm.idata.posterior.sizes[\"draw\"]\n",
    "    n_chains = mmm.idata.posterior.sizes[\"chain\"]\n",
    "    total_samples = n_draws * n_chains\n",
    "    divergence_rate = n_divergences / total_samples\n",
    "    \n",
    "    # 2. Check R-hat\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\".*invalid value encountered.*\")\n",
    "        rhat = az.rhat(mmm.idata)\n",
    "        max_rhat = float(rhat.to_array().max())\n",
    "    \n",
    "    # 3. Check ESS\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\".*invalid value encountered.*\")\n",
    "        ess = az.ess(mmm.idata)\n",
    "        min_ess = float(ess.to_array().min())\n",
    "    \n",
    "    # Convergence checks\n",
    "    divergence_ok = divergence_rate <= divergence_threshold\n",
    "    rhat_ok = max_rhat <= rhat_threshold\n",
    "    ess_ok = min_ess >= ess_threshold\n",
    "    \n",
    "    converged = divergence_ok and rhat_ok and ess_ok\n",
    "    \n",
    "    # Diagnostics dictionary\n",
    "    diagnostics = {\n",
    "        \"n_divergences\": n_divergences,\n",
    "        \"divergence_rate\": divergence_rate,\n",
    "        \"max_rhat\": max_rhat,\n",
    "        \"min_ess\": min_ess,\n",
    "    }\n",
    "    \n",
    "    # Logging\n",
    "    trial_str = f\"Trial {trial_number}\" if trial_number is not None else \"Model\"\n",
    "    \n",
    "    if converged:\n",
    "        rprint(\n",
    "            f\"[green]✓ {trial_str} CONVERGED:[/green] \"\n",
    "            f\"divergences={divergence_rate*100:.1f}%, \"\n",
    "            f\"max_rhat={max_rhat:.3f}, \"\n",
    "            f\"min_ess={min_ess:.0f}\"\n",
    "        )\n",
    "    else:\n",
    "        reasons = []\n",
    "        if not divergence_ok:\n",
    "            reasons.append(f\"divergences={divergence_rate*100:.1f}% (>{divergence_threshold*100:.0f}%)\")\n",
    "        if not rhat_ok:\n",
    "            reasons.append(f\"max_rhat={max_rhat:.3f} (>{rhat_threshold})\")\n",
    "        if not ess_ok:\n",
    "            reasons.append(f\"min_ess={min_ess:.0f} (<{ess_threshold})\")\n",
    "        \n",
    "        rprint(\n",
    "            f\"[red]✗ {trial_str} FAILED:[/red] \"\n",
    "            f\"{', '.join(reasons)}\"\n",
    "        )\n",
    "    \n",
    "    return converged, diagnostics\n",
    "\n",
    "\n",
    "rprint(\"[bold green]Convergence check function defined[/bold green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRPS Evaluation Function\n",
    "\n",
    "Compute CRPS on test set using posterior predictive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_test_crps(\n    mmm: MMM,\n    X_test: pd.DataFrame,\n    y_test: pd.Series\n) -> float:\n    \"\"\"Compute CRPS on test set using posterior predictive.\n    \n    Args:\n        mmm: Fitted MMM model\n        X_test: Test features\n        y_test: Test target values\n        \n    Returns:\n        CRPS score (lower is better)\n    \"\"\"\n    # Sample posterior predictive for test set\n    # Note: original_scale=True doesn't rescale posterior_predictive samples\n    mmm.sample_posterior_predictive(\n        X_test,\n        original_scale=True,\n        extend_idata=True\n    )\n    \n    # Extract posterior predictive samples (in normalized scale)\n    # Shape: (n_chains, n_draws, n_observations)\n    y_pred_samples = mmm.idata.posterior_predictive[\"y\"].values\n    \n    # Manually rescale to original scale\n    target_scale = float(mmm.idata.constant_data[\"target_scale\"].values)\n    y_pred_rescaled = y_pred_samples * target_scale\n    \n    # Reshape to (n_samples, n_observations) as expected by crps()\n    # n_samples = n_chains * n_draws\n    n_chains, n_draws, n_obs = y_pred_rescaled.shape\n    y_pred_reshaped = y_pred_rescaled.reshape(n_chains * n_draws, n_obs)\n    \n    # Compute CRPS\n    y_test_array = y_test.values\n    crps_score = crps(y_test_array, y_pred_reshaped)\n    \n    return float(crps_score)\n\n\nrprint(\"[bold green]CRPS evaluation function defined[/bold green]\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optuna Objective Function\n",
    "\n",
    "The objective function:\n",
    "1. Suggests hyperparameters from search space\n",
    "2. Creates and fits MMM model on **train set**\n",
    "3. Checks convergence on train set\n",
    "4. Computes CRPS on **test set** (lower is better)\n",
    "5. Returns CRPS for Optuna to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_objective(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    channel_columns: list[str],\n",
    "    control_columns: list[str],\n",
    "    draws: int,\n",
    "    tune: int,\n",
    "    chains: int,\n",
    "    divergence_threshold: float,\n",
    "    rhat_threshold: float,\n",
    "    ess_threshold: float,\n",
    "    nuts_sampler: str = \"pymc\"\n",
    ") -> callable:\n",
    "    \"\"\"Create Optuna objective function using CRPS on test set.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training target\n",
    "        X_test: Test features\n",
    "        y_test: Test target\n",
    "        channel_columns: List of channel column names\n",
    "        control_columns: List of control column names\n",
    "        draws: Number of MCMC draws\n",
    "        tune: Number of tuning steps\n",
    "        chains: Number of MCMC chains\n",
    "        divergence_threshold: Maximum allowed divergence rate\n",
    "        rhat_threshold: Maximum allowed R-hat value\n",
    "        ess_threshold: Minimum required effective sample size\n",
    "        nuts_sampler: NUTS sampler to use\n",
    "        \n",
    "    Returns:\n",
    "        Objective function that takes Optuna trial and returns test CRPS\n",
    "    \"\"\"\n",
    "    def objective(trial: optuna.Trial) -> float:\n",
    "        \"\"\"Optuna objective function using CRPS on test set.\n",
    "        \n",
    "        Args:\n",
    "            trial: Optuna trial object\n",
    "            \n",
    "        Returns:\n",
    "            Test CRPS value (lower is better)\n",
    "            \n",
    "        Raises:\n",
    "            optuna.TrialPruned: If model fails to converge\n",
    "        \"\"\"\n",
    "        # Suggest hyperparameters\n",
    "        yearly_seasonality = trial.suggest_int(\"yearly_seasonality\", 1, 10)\n",
    "        adstock_max_lag = trial.suggest_int(\"adstock_max_lag\", 4, 12)\n",
    "        \n",
    "        rprint(\n",
    "            f\"\\n[bold cyan]Trial {trial.number}:[/bold cyan] \"\n",
    "            f\"yearly_seasonality={yearly_seasonality}, \"\n",
    "            f\"adstock_max_lag={adstock_max_lag}\"\n",
    "        )\n",
    "        \n",
    "        # Create MMM model\n",
    "        mmm = MMM(\n",
    "            date_column=\"date\",\n",
    "            channel_columns=channel_columns,\n",
    "            control_columns=control_columns,\n",
    "            adstock=GeometricAdstock(l_max=adstock_max_lag),\n",
    "            saturation=LogisticSaturation(),\n",
    "            yearly_seasonality=yearly_seasonality\n",
    "        )\n",
    "        \n",
    "        # Fit model on TRAINING set\n",
    "        mmm.fit(\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            draws=draws,\n",
    "            tune=tune,\n",
    "            chains=chains,\n",
    "            nuts_sampler=nuts_sampler,\n",
    "            random_seed=RANDOM_SEED + trial.number,\n",
    "            progressbar=False\n",
    "        )\n",
    "        \n",
    "        # Check convergence\n",
    "        converged, diagnostics = check_convergence(\n",
    "            mmm=mmm,\n",
    "            divergence_threshold=divergence_threshold,\n",
    "            rhat_threshold=rhat_threshold,\n",
    "            ess_threshold=ess_threshold,\n",
    "            trial_number=trial.number\n",
    "        )\n",
    "        \n",
    "        # Store diagnostics\n",
    "        trial.set_user_attr(\"n_divergences\", diagnostics[\"n_divergences\"])\n",
    "        trial.set_user_attr(\"divergence_rate\", diagnostics[\"divergence_rate\"])\n",
    "        trial.set_user_attr(\"max_rhat\", diagnostics[\"max_rhat\"])\n",
    "        trial.set_user_attr(\"min_ess\", diagnostics[\"min_ess\"])\n",
    "        trial.set_user_attr(\"converged\", converged)\n",
    "        \n",
    "        # Prune if not converged\n",
    "        if not converged:\n",
    "            rprint(f\"[red]Trial {trial.number} pruned due to convergence failure[/red]\")\n",
    "            raise optuna.TrialPruned()\n",
    "        \n",
    "        # Compute CRPS on TEST set\n",
    "        test_crps = compute_test_crps(mmm, X_test, y_test)\n",
    "        \n",
    "        rprint(f\"[yellow]Test CRPS: {test_crps:.2f}[/yellow]\")\n",
    "        \n",
    "        # Store CRPS\n",
    "        trial.set_user_attr(\"test_crps\", test_crps)\n",
    "        \n",
    "        return test_crps\n",
    "    \n",
    "    return objective\n",
    "\n",
    "\n",
    "# Create objective function\n",
    "objective = create_objective(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    channel_columns=channel_columns,\n",
    "    control_columns=control_columns,\n",
    "    draws=OPTUNA_DRAWS,\n",
    "    tune=OPTUNA_TUNE,\n",
    "    chains=OPTUNA_CHAINS,\n",
    "    divergence_threshold=OPTUNA_DIVERGENCE_THRESHOLD,\n",
    "    rhat_threshold=OPTUNA_RHAT_THRESHOLD,\n",
    "    ess_threshold=OPTUNA_ESS_THRESHOLD,\n",
    "    nuts_sampler=NUTS_SAMPLER\n",
    ")\n",
    "\n",
    "rprint(\"[bold green]Objective function created with CRPS evaluation[/bold green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Optuna Optimization\n",
    "\n",
    "This will run multiple trials to find hyperparameters that minimize test CRPS.\n",
    "\n",
    "**Note**: This may take 15-30 minutes depending on `N_TRIALS` and hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(\n",
    "    objective: callable,\n",
    "    n_trials: int,\n",
    "    study_name: str = \"mmm_crps_optimization\"\n",
    ") -> optuna.Study:\n",
    "    \"\"\"Run Optuna hyperparameter optimization.\n",
    "    \n",
    "    Args:\n",
    "        objective: Objective function to minimize\n",
    "        n_trials: Number of optimization trials\n",
    "        study_name: Name for the Optuna study\n",
    "        \n",
    "    Returns:\n",
    "        Completed Optuna study object\n",
    "    \"\"\"\n",
    "    rprint(f\"\\n[bold magenta]Starting CRPS-based optimization with {n_trials} trials...[/bold magenta]\\n\")\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        direction=\"minimize\",  # Minimize test CRPS\n",
    "        sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),\n",
    "        pruner=optuna.pruners.MedianPruner(\n",
    "            n_startup_trials=5,\n",
    "            n_warmup_steps=0\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Run optimization\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=n_trials,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    rprint(\"\\n[bold green]Optimization completed![/bold green]\")\n",
    "    \n",
    "    return study\n",
    "\n",
    "\n",
    "# Run optimization\n",
    "study = run_optimization(\n",
    "    objective=objective,\n",
    "    n_trials=N_TRIALS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_optimization_results(study: optuna.Study) -> None:\n",
    "    \"\"\"Display optimization results in a Rich table.\n",
    "    \n",
    "    Args:\n",
    "        study: Completed Optuna study\n",
    "    \"\"\"\n",
    "    console = Console()\n",
    "    \n",
    "    # Best trial summary\n",
    "    rprint(\"\\n[bold magenta]Best Trial:[/bold magenta]\")\n",
    "    rprint(f\"Trial number: [cyan]{study.best_trial.number}[/cyan]\")\n",
    "    rprint(f\"Test CRPS: [yellow]{study.best_value:.2f}[/yellow]\")\n",
    "    rprint(f\"Parameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        rprint(f\"  - {key}: [green]{value}[/green]\")\n",
    "    \n",
    "    # All trials table\n",
    "    rprint(\"\\n[bold blue]All Trials:[/bold blue]\")\n",
    "    table = Table(\n",
    "        title=\"Optimization Trials (CRPS-based)\",\n",
    "        show_header=True,\n",
    "        header_style=\"bold cyan\"\n",
    "    )\n",
    "    \n",
    "    table.add_column(\"Trial\", justify=\"right\")\n",
    "    table.add_column(\"Yearly Seasonality\", justify=\"right\")\n",
    "    table.add_column(\"Adstock Max Lag\", justify=\"right\")\n",
    "    table.add_column(\"Test CRPS\", justify=\"right\")\n",
    "    table.add_column(\"Status\")\n",
    "    \n",
    "    for trial in study.trials:\n",
    "        status_color = \"green\" if trial.state == optuna.trial.TrialState.COMPLETE else \"red\"\n",
    "        crps_str = f\"{trial.value:.2f}\" if trial.value is not None else \"N/A\"\n",
    "        \n",
    "        table.add_row(\n",
    "            str(trial.number),\n",
    "            str(trial.params.get(\"yearly_seasonality\", \"N/A\")),\n",
    "            str(trial.params.get(\"adstock_max_lag\", \"N/A\")),\n",
    "            crps_str,\n",
    "            f\"[{status_color}]{trial.state.name}[/{status_color}]\"\n",
    "        )\n",
    "    \n",
    "    console.print(table)\n",
    "\n",
    "\n",
    "display_optimization_results(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from optuna.visualization.matplotlib import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_contour\n",
    ")\n",
    "\n",
    "\n",
    "def plot_optimization_results(study: optuna.Study) -> None:\n",
    "    \"\"\"Plot optimization results.\n",
    "    \n",
    "    Args:\n",
    "        study: Completed Optuna study\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Optimization history\n",
    "    plt.sca(axes[0, 0])\n",
    "    plot_optimization_history(study)\n",
    "    axes[0, 0].set_title(\"Optimization History (Test CRPS)\")\n",
    "    \n",
    "    # Parameter importances\n",
    "    plt.sca(axes[0, 1])\n",
    "    plot_param_importances(study)\n",
    "    axes[0, 1].set_title(\"Parameter Importances\")\n",
    "    \n",
    "    # Contour plot\n",
    "    plt.sca(axes[1, 0])\n",
    "    plot_contour(study, params=[\"yearly_seasonality\", \"adstock_max_lag\"])\n",
    "    axes[1, 0].set_title(\"Parameter Relationship\")\n",
    "    \n",
    "    # CRPS distribution\n",
    "    crps_values = [t.value for t in study.trials if t.value is not None]\n",
    "    axes[1, 1].hist(crps_values, bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[1, 1].axvline(study.best_value, color='red', linestyle='--', label='Best Test CRPS')\n",
    "    axes[1, 1].set_xlabel('Test CRPS')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Test CRPS Distribution')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_optimization_results(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Final Model on Full Dataset\n",
    "\n",
    "Now refit the model with optimal parameters on the **full dataset** (all 104 weeks) using production-quality MCMC settings.\n",
    "\n",
    "**Note**: Progress bar disabled for NumPyro to avoid Jupyter kernel issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_final_model(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    channel_columns: list[str],\n",
    "    control_columns: list[str],\n",
    "    best_params: dict[str, Any],\n",
    "    draws: int,\n",
    "    tune: int,\n",
    "    chains: int,\n",
    "    random_seed: int,\n",
    "    divergence_threshold: float,\n",
    "    rhat_threshold: float,\n",
    "    ess_threshold: float,\n",
    "    nuts_sampler: str = \"pymc\"\n",
    ") -> MMM:\n",
    "    \"\"\"Fit final MMM model on full dataset with optimal hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        X: Full dataset features\n",
    "        y: Full dataset target\n",
    "        channel_columns: List of channel column names\n",
    "        control_columns: List of control column names\n",
    "        best_params: Dictionary of optimal hyperparameters\n",
    "        draws: Number of MCMC draws\n",
    "        tune: Number of tuning steps\n",
    "        chains: Number of MCMC chains\n",
    "        random_seed: Random seed for reproducibility\n",
    "        divergence_threshold: Maximum allowed divergence rate\n",
    "        rhat_threshold: Maximum allowed R-hat value\n",
    "        ess_threshold: Minimum required effective sample size\n",
    "        nuts_sampler: NUTS sampler to use\n",
    "        \n",
    "    Returns:\n",
    "        Fitted MMM model\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: If final model fails to converge\n",
    "    \"\"\"\n",
    "    rprint(\"\\n[bold magenta]Fitting final model on FULL dataset with optimal hyperparameters...[/bold magenta]\")\n",
    "    rprint(f\"yearly_seasonality: [green]{best_params['yearly_seasonality']}[/green]\")\n",
    "    rprint(f\"adstock_max_lag: [green]{best_params['adstock_max_lag']}[/green]\")\n",
    "    rprint(f\"MCMC settings: {draws} draws, {tune} tune, {chains} chains\")\n",
    "    rprint(f\"\\n[bold yellow]Convergence requirements (strict):[/bold yellow]\")\n",
    "    rprint(f\"Divergences: <{divergence_threshold*100:.0f}%, R-hat: <{rhat_threshold}, ESS: >{ess_threshold}\\n\")\n",
    "    \n",
    "    # Create model\n",
    "    mmm = MMM(\n",
    "        date_column=\"date\",\n",
    "        channel_columns=channel_columns,\n",
    "        control_columns=control_columns,\n",
    "        adstock=GeometricAdstock(l_max=best_params[\"adstock_max_lag\"]),\n",
    "        saturation=LogisticSaturation(),\n",
    "        yearly_seasonality=best_params[\"yearly_seasonality\"]\n",
    "    )\n",
    "    \n",
    "    # Fit model on FULL dataset\n",
    "    mmm.fit(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        draws=draws,\n",
    "        tune=tune,\n",
    "        chains=chains,\n",
    "        nuts_sampler=nuts_sampler,\n",
    "        random_seed=random_seed,\n",
    "        progressbar=False if nuts_sampler == \"numpyro\" else True\n",
    "    )\n",
    "    \n",
    "    # Check convergence\n",
    "    converged, diagnostics = check_convergence(\n",
    "        mmm=mmm,\n",
    "        divergence_threshold=divergence_threshold,\n",
    "        rhat_threshold=rhat_threshold,\n",
    "        ess_threshold=ess_threshold\n",
    "    )\n",
    "    \n",
    "    if not converged:\n",
    "        error_msg = (\n",
    "            f\"Final model failed to converge:\\n\"\n",
    "            f\"  Divergences: {diagnostics['divergence_rate']*100:.1f}% (threshold: <{divergence_threshold*100:.0f}%)\\n\"\n",
    "            f\"  Max R-hat: {diagnostics['max_rhat']:.3f} (threshold: <{rhat_threshold})\\n\"\n",
    "            f\"  Min ESS: {diagnostics['min_ess']:.0f} (threshold: >{ess_threshold})\\n\"\n",
    "        )\n",
    "        rprint(f\"[bold red]{error_msg}[/bold red]\")\n",
    "        raise RuntimeError(error_msg)\n",
    "    \n",
    "    rprint(\"[bold green]Final model fitted and converged successfully![/bold green]\")\n",
    "    \n",
    "    return mmm\n",
    "\n",
    "\n",
    "# Prepare full dataset\n",
    "df_full_pandas = df.to_pandas()\n",
    "X_full = df_full_pandas.drop(columns=[\"y\"])\n",
    "y_full = df_full_pandas[\"y\"]\n",
    "\n",
    "# Fit final model\n",
    "final_mmm = fit_final_model(\n",
    "    X=X_full,\n",
    "    y=y_full,\n",
    "    channel_columns=channel_columns,\n",
    "    control_columns=control_columns,\n",
    "    best_params=study.best_params,\n",
    "    draws=FINAL_DRAWS,\n",
    "    tune=FINAL_TUNE,\n",
    "    chains=FINAL_CHAINS,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    divergence_threshold=FINAL_DIVERGENCE_THRESHOLD,\n",
    "    rhat_threshold=FINAL_RHAT_THRESHOLD,\n",
    "    ess_threshold=FINAL_ESS_THRESHOLD,\n",
    "    nuts_sampler=NUTS_SAMPLER\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display fit summary\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in scalar divide\")\n",
    "    summary = az.summary(final_mmm.idata)\n",
    "\n",
    "rprint(\"[bold magenta]Final Model Fit Summary:[/bold magenta]\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Final Model on Train and Test Sets\n",
    "\n",
    "Compute CRPS on both train and test sets to assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_final_model(\n",
    "    mmm: MMM,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Evaluate final model on train and test sets.\n",
    "    \n",
    "    Args:\n",
    "        mmm: Fitted MMM model on full dataset\n",
    "        X_train: Training features\n",
    "        y_train: Training target\n",
    "        X_test: Test features\n",
    "        y_test: Test target\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with train and test CRPS scores\n",
    "    \"\"\"\n",
    "    rprint(\"\\n[bold blue]Evaluating final model on train and test sets...[/bold blue]\")\n",
    "    \n",
    "    # Evaluate on train set\n",
    "    rprint(\"Computing train CRPS...\")\n",
    "    train_crps = compute_test_crps(mmm, X_train, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    rprint(\"Computing test CRPS...\")\n",
    "    test_crps = compute_test_crps(mmm, X_test, y_test)\n",
    "    \n",
    "    # Display results\n",
    "    console = Console()\n",
    "    table = Table(\n",
    "        title=\"Final Model CRPS Evaluation\",\n",
    "        show_header=True,\n",
    "        header_style=\"bold green\"\n",
    "    )\n",
    "    \n",
    "    table.add_column(\"Dataset\", style=\"cyan\")\n",
    "    table.add_column(\"CRPS\", justify=\"right\")\n",
    "    table.add_column(\"Note\")\n",
    "    \n",
    "    table.add_row(\"Train\", f\"{train_crps:.2f}\", \"Expected to be lower (fitted on this data)\")\n",
    "    table.add_row(\"Test\", f\"{test_crps:.2f}\", \"Generalization performance\")\n",
    "    \n",
    "    console.print(\"\\n\")\n",
    "    console.print(table)\n",
    "    \n",
    "    return {\"train_crps\": train_crps, \"test_crps\": test_crps}\n",
    "\n",
    "\n",
    "# Evaluate final model\n",
    "evaluation_results = evaluate_final_model(\n",
    "    mmm=final_mmm,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Predictions\n",
    "\n",
    "Plot posterior predictive samples vs actual values for train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\n\ndef plot_predictions(\n    mmm: MMM,\n    df_train: pl.DataFrame,\n    df_test: pl.DataFrame,\n    X_train: pd.DataFrame,\n    X_test: pd.DataFrame\n) -> None:\n    \"\"\"Plot model predictions vs actual values.\n    \n    Args:\n        mmm: Fitted MMM model\n        df_train: Training dataframe with dates and actuals\n        df_test: Test dataframe with dates and actuals\n        X_train: Training features\n        X_test: Test features\n    \"\"\"\n    rprint(\"\\n[bold blue]Generating prediction plots...[/bold blue]\")\n    \n    # Get target scale for manual rescaling\n    target_scale = float(mmm.idata.constant_data[\"target_scale\"].values)\n    \n    # Sample posterior predictive for train and test\n    mmm.sample_posterior_predictive(X_train, original_scale=True, extend_idata=False)\n    train_pred = mmm.idata.posterior_predictive[\"y\"].values * target_scale\n    \n    mmm.sample_posterior_predictive(X_test, original_scale=True, extend_idata=False)\n    test_pred = mmm.idata.posterior_predictive[\"y\"].values * target_scale\n    \n    # Compute mean and credible intervals\n    train_pred_mean = train_pred.mean(axis=(0, 1))\n    train_pred_lower = np.percentile(train_pred, 2.5, axis=(0, 1))\n    train_pred_upper = np.percentile(train_pred, 97.5, axis=(0, 1))\n    \n    test_pred_mean = test_pred.mean(axis=(0, 1))\n    test_pred_lower = np.percentile(test_pred, 2.5, axis=(0, 1))\n    test_pred_upper = np.percentile(test_pred, 97.5, axis=(0, 1))\n    \n    # Get dates and actuals\n    train_dates = df_train[\"date\"].to_numpy()\n    train_actuals = df_train[\"y\"].to_numpy()\n    test_dates = df_test[\"date\"].to_numpy()\n    test_actuals = df_test[\"y\"].to_numpy()\n    \n    # Plot\n    fig, ax = plt.subplots(figsize=(15, 6))\n    \n    # Train set\n    ax.plot(train_dates, train_actuals, 'o-', color='black', alpha=0.6, label='Train Actual', markersize=4)\n    ax.plot(train_dates, train_pred_mean, '-', color='blue', label='Train Prediction', linewidth=2)\n    ax.fill_between(train_dates, train_pred_lower, train_pred_upper, color='blue', alpha=0.2, label='Train 95% CI')\n    \n    # Test set\n    ax.plot(test_dates, test_actuals, 's-', color='darkred', alpha=0.8, label='Test Actual', markersize=5)\n    ax.plot(test_dates, test_pred_mean, '-', color='red', label='Test Prediction', linewidth=2)\n    ax.fill_between(test_dates, test_pred_lower, test_pred_upper, color='red', alpha=0.2, label='Test 95% CI')\n    \n    # Vertical line separating train/test\n    ax.axvline(train_dates[-1], color='gray', linestyle='--', linewidth=2, alpha=0.5, label='Train/Test Split')\n    \n    ax.set_xlabel('Date', fontsize=12)\n    ax.set_ylabel('Sales', fontsize=12)\n    ax.set_title('MMM Predictions: Train vs Test Set', fontsize=14, fontweight='bold')\n    ax.legend(loc='upper left', fontsize=10)\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    rprint(\"[bold green]Prediction plot generated successfully![/bold green]\")\n\n\n# Plot predictions\nplot_predictions(\n    mmm=final_mmm,\n    df_train=df_train,\n    df_test=df_test,\n    X_train=X_train,\n    X_test=X_test\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute ROAS and Compare with Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def compute_and_compare_roas(\n",
    "    model: MMM,\n",
    "    channel_spend: pd.DataFrame,\n",
    "    channel_columns: list[str],\n",
    "    ground_truth_path: str | Path\n",
    ") -> None:\n",
    "    \"\"\"Compute ROAS and compare with ground truth.\n",
    "    \n",
    "    Args:\n",
    "        model: Fitted MMM model\n",
    "        channel_spend: DataFrame with channel spend data\n",
    "        channel_columns: List of channel column names\n",
    "        ground_truth_path: Path to ground truth parameters JSON\n",
    "    \"\"\"\n",
    "    # Compute channel contributions\n",
    "    contributions = model.compute_mean_contributions_over_time()\n",
    "    \n",
    "    # Compute ROAS\n",
    "    total_contributions = contributions[channel_columns].sum()\n",
    "    total_spend = channel_spend[channel_columns].sum()\n",
    "    roas = total_contributions / total_spend\n",
    "    \n",
    "    # Load ground truth\n",
    "    with open(ground_truth_path) as f:\n",
    "        ground_truth = json.load(f)\n",
    "    \n",
    "    true_roas = ground_truth[\"roas_values\"][\"Local\"]\n",
    "    \n",
    "    # Create comparison table\n",
    "    console = Console()\n",
    "    table = Table(\n",
    "        title=\"ROAS Comparison: Estimated vs Ground Truth\",\n",
    "        show_header=True,\n",
    "        header_style=\"bold green\"\n",
    "    )\n",
    "    \n",
    "    table.add_column(\"Channel\", style=\"cyan\")\n",
    "    table.add_column(\"Estimated ROAS\", justify=\"right\")\n",
    "    table.add_column(\"True ROAS\", justify=\"right\")\n",
    "    table.add_column(\"Error %\", justify=\"right\")\n",
    "    \n",
    "    for channel in channel_columns:\n",
    "        # Extract base channel name\n",
    "        channel_name = channel.split('_', 1)[1] if '_' in channel else channel\n",
    "        \n",
    "        est_val = roas[channel]\n",
    "        true_val = true_roas.get(channel_name, 0.0)\n",
    "        error_pct = ((est_val - true_val) / true_val * 100) if true_val != 0 else 0.0\n",
    "        \n",
    "        table.add_row(\n",
    "            channel_name,\n",
    "            f\"{est_val:.2f}\",\n",
    "            f\"{true_val:.2f}\",\n",
    "            f\"{error_pct:+.1f}%\"\n",
    "        )\n",
    "    \n",
    "    console.print(\"\\n\")\n",
    "    console.print(table)\n",
    "\n",
    "\n",
    "# Compute and compare ROAS\n",
    "ground_truth_path = Path(\"../data/ground_truth_parameters.json\")\n",
    "compute_and_compare_roas(\n",
    "    model=final_mmm,\n",
    "    channel_spend=df_full_pandas,\n",
    "    channel_columns=channel_columns,\n",
    "    ground_truth_path=ground_truth_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def save_optimization_results(\n",
    "    study: optuna.Study,\n",
    "    evaluation_results: dict[str, float],\n",
    "    output_dir: str | Path\n",
    ") -> None:\n",
    "    \"\"\"Save optimization results to disk.\n",
    "    \n",
    "    Args:\n",
    "        study: Completed Optuna study\n",
    "        evaluation_results: Dictionary with train/test CRPS\n",
    "        output_dir: Directory to save results\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save best parameters\n",
    "    best_params_path = output_dir / \"best_hyperparameters_crps.json\"\n",
    "    with open(best_params_path, \"w\") as f:\n",
    "        json.dump(study.best_params, f, indent=2)\n",
    "    \n",
    "    # Save trials dataframe\n",
    "    trials_df = study.trials_dataframe()\n",
    "    trials_path = output_dir / \"optimization_trials_crps.csv\"\n",
    "    trials_df.to_csv(trials_path, index=False)\n",
    "    \n",
    "    # Save evaluation results\n",
    "    eval_path = output_dir / \"final_evaluation_crps.json\"\n",
    "    with open(eval_path, \"w\") as f:\n",
    "        json.dump(evaluation_results, f, indent=2)\n",
    "    \n",
    "    rprint(f\"\\n[bold green]Results saved to {output_dir}[/bold green]\")\n",
    "    rprint(f\"  - Best parameters: {best_params_path}\")\n",
    "    rprint(f\"  - Trial history: {trials_path}\")\n",
    "    rprint(f\"  - Final evaluation: {eval_path}\")\n",
    "\n",
    "\n",
    "def save_final_model(model: MMM, output_path: str | Path) -> None:\n",
    "    \"\"\"Save final fitted model to disk.\n",
    "    \n",
    "    Args:\n",
    "        model: Fitted MMM model\n",
    "        output_path: Path to save model\n",
    "    \"\"\"\n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model.save(str(output_path))\n",
    "    rprint(f\"[bold green]Final model saved to {output_path}[/bold green]\")\n",
    "\n",
    "\n",
    "# Save results\n",
    "save_optimization_results(\n",
    "    study=study,\n",
    "    evaluation_results=evaluation_results,\n",
    "    output_dir=Path(\"../models/optimization_results\")\n",
    ")\n",
    "save_final_model(final_mmm, output_path=Path(\"../models/mmm_optimized_crps.nc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Train/test split**: Chronological split with last 24 weeks as test set\n",
    "2. **CRPS-based optimization**: Using test set performance instead of WAIC\n",
    "3. **Hyperparameter search**: Optimizing `yearly_seasonality` and `adstock_max_lag`\n",
    "4. **Convergence monitoring**: Strict checks on MCMC diagnostics\n",
    "5. **Final model**: Refit on full dataset with optimal parameters\n",
    "6. **Evaluation**: Train and test CRPS comparison\n",
    "7. **Visualization**: Predictions vs actuals for both sets\n",
    "8. **ROAS validation**: Comparison with known ground truth\n",
    "\n",
    "### Key Differences from WAIC Approach\n",
    "\n",
    "| Aspect | WAIC (Notebook 02) | CRPS (This Notebook) |\n",
    "|--------|-------------------|----------------------|\n",
    "| **Objective** | In-sample fit (penalized) | Out-of-sample prediction |\n",
    "| **Data used** | Full training set | Held-out test set |\n",
    "| **Evaluation** | Log predictive density | Proper scoring rule |\n",
    "| **Advantage** | No data loss | Direct generalization metric |\n",
    "| **Disadvantage** | Indirect measure | Less training data |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Compare optimal hyperparameters from WAIC vs CRPS approaches\n",
    "- Implement k-fold time-series cross-validation\n",
    "- Extend to optimize saturation function hyperparameters\n",
    "- Add budget allocation optimization with optimal model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}